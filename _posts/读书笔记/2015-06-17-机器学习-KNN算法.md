---
title: 机器学习-KNN算法
layout: post
category: 机器学习
---

###k-NN简介

k-NN(k-Nearest Neihbors)最近邻规则，是一种处理分类问题的一种算法。

KNN算法寻找训练样本集中与输入样本最邻近的k个样本作为输入样本分类的确定依据，用这k个样本中分类最多的那个分类表示输入样本的分类。
当k=1时，即寻找与输入样本最近的那个样本点，这样分类的结果就会对局部特征非常敏感，当k=N时，那么输入样本的类别极为训练样本集中大多数
样本所属的分类，可见k值过大过小都会对分类结果产生影响，适当增大k值可以平滑掉由于当个数据点的噪声而引起的波动性。
一般来说k值为十几或几十。

###k-NN的距离度量

在设计最近邻分类器时，需要一个衡量样本之间距离的度量函数，该函数能够给出两个样本之间标量距离的大小。一个距离度量函数
D(.,.)必须满足4个性质，即对任意的向量a, b, c有：

> (1) 非负性: D(a, a) >= 0;
> (2) 自反性：D(a, b) = 0,当且仅当 a = b;
> (3) 对称性: D(a, b) = D(b, a);
> (4) 三角不等式: D(a, b) + D(b, c) >= D(a, c).

容易证明，d维空间的欧几里得距离满足上述性质。在监督分类问题中，包含l个属性的两个样本
X = (x1, x2, ..., xl)和Z = (z1, z2, ..., zl)之间的欧氏距离定义为：

<a href="http://www.codecogs.com/eqnedit.php?latex=D(X,Z)&space;=&space;\sqrt{$$\sum_{i=1}^{n}&space;(x_{i}&space;-&space;z{i})$$}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?D(X,Z)&space;=&space;\sqrt{$$\sum_{i=1}^{n}&space;(x_{i}&space;-&space;z{i})$$}" title="D(X,Z) = \sqrt{$$\sum_{i=1}^{n} (x_{i} - z{i})$$}" /></a>
